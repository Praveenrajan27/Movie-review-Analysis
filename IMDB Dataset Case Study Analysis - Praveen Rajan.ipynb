{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # IMDB Dataset Case Study Analysis - Exploratory Analysis and ML Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Introduction: </b>\n",
    "A commercial success movie not only entertains audience, but also enables film companies to gain tremendous profit. A lot of factors such as good directors, experienced actors are considerable for creating good movies. However, famous directors and actors can always bring an expected box-office income but cannot guarantee a highly rated imdb score.\n",
    "\n",
    "<b>Data Description:</b>\n",
    "The dataset (movie-review-data.csv) contains 28 variables for 5043 movies, spanning across 100 years in 66 countries. There are 2399 unique director names, and thousands of actors/actresses. “imdb_score” is the response variable while the other 27 variables are possible predictors.\n",
    "\n",
    "<b>Problem Statement:</b>\n",
    "Build Model to predict what kind of movies are more successful.\n",
    "Take imdb scores as response variable and focus on operating predictions by analyzing the rest of variables in the movie data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Exploratory Data Analysis](#id1)\n",
    "\n",
    "    1.1 [Likes of Movie and Director on Facebook gives a good leverage](#id1.1) \n",
    "    \n",
    "    1.2 [Cast and Actor Popularity](#id1.2) \n",
    "    \n",
    "    1.3 [Does Title year and duration of Movie impact scores?](#id1.3) \n",
    "    \n",
    "    1.4 [High Budget and Gross influences Scores](#id1.4) \n",
    "    \n",
    "    1.5 [Other fetaures impacting Scores](#id1.5) \n",
    "\n",
    "\n",
    "2. [Data cleaning and preprocessing](#id2)\n",
    "\n",
    "    2.1 [Handling NAs](#id2.1) \n",
    "    \n",
    "    2.2 [Feature Engineering](#id2.2) \n",
    "\n",
    "\n",
    "3. [Baseline Model](#id3)\n",
    "\n",
    "\n",
    "4. [Model building and Metrics](#id4)\n",
    "\n",
    "    4.1 [Decision Tree](#id4.1) \n",
    "    \n",
    "    4.2 [Random Forest Model](#id4.2) \n",
    "    \n",
    "    4.3 [Gradient Boosting Model](#id4.3) \n",
    "    \n",
    "    4.4 [Cat Boost Model](#id4.4) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "5. [Conclusion](#id5)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from astropy.visualization import hist\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\rajanp\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (1.5.4)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.6.1-cp37-cp37m-win_amd64.whl (32.6 MB)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\rajanp\\appdata\\roaming\\python\\python37\\site-packages (from scipy) (1.20.1)\n",
      "Installing collected packages: scipy\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.5.4\n",
      "    Uninstalling scipy-1.5.4:\n",
      "      Successfully uninstalled scipy-1.5.4\n",
      "Successfully installed scipy-1.6.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "phik 0.10.0 requires numba>=0.38.1, which is not installed.\n",
      "imagehash 4.2.0 requires PyWavelets, which is not installed.\n",
      "spherecluster 0.1.7 requires nose, which is not installed.\n",
      "shap 0.28.3 requires scikit-image, which is not installed.\n",
      "sentence-transformers 0.3.7.2 requires nltk, which is not installed.\n",
      "pyldavis 2.1.2 requires numexpr, which is not installed.\n",
      "lime 0.1.1.32 requires scikit-image>=0.12, which is not installed.\n",
      "ktrain 0.21.2 requires bokeh, which is not installed.\n",
      "keras 2.4.3 requires h5py, which is not installed.\n",
      "flair 0.6.1 requires lxml, which is not installed.\n",
      "exploripy 1.0.3 requires statsmodels, which is not installed.\n",
      "allennlp 0.8.3 requires flask>=1.0.2, which is not installed.\n",
      "allennlp 0.8.3 requires gevent>=1.3.6, which is not installed.\n",
      "allennlp 0.8.3 requires h5py, which is not installed.\n",
      "allennlp 0.8.3 requires msgpack<0.6.0,>=0.5.6, which is not installed.\n",
      "allennlp 0.8.3 requires nltk, which is not installed.\n",
      "allennlp 0.8.3 requires numpydoc>=0.8.0, which is not installed.\n",
      "phik 0.10.0 requires joblib>=0.14.1, but you have joblib 0.13.1 which is incompatible.\n",
      "allennlp 0.8.3 requires spacy<2.2,>=2.0, but you have spacy 2.2.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -U scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'issparse' from 'scipy.sparse' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-36ae29d142cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mplot_confusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m from .utils._tags import (\n\u001b[0;32m     19\u001b[0m     \u001b[0m_DEFAULT_TAGS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmurmurhash\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmurmurhash3_32\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'issparse' from 'scipy.sparse' (unknown location)"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix,plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('movie_review_data.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['movie_imdb_link'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup=df['movie_imdb_link'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_titles=list(dup[dup>1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['movie_imdb_link'].isin(duplicate_titles)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['movie_imdb_link'].isin(duplicate_titles),['movie_title','movie_imdb_link']].head(5).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['movie_imdb_link']=='http://www.imdb.com/title/tt0413300/?ref_=fn_tt_tt_1',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.random((1000, 1000))\n",
    "y = np.random.random((1000, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start=time()\n",
    "z=np.matmul(x,y)\n",
    "t_end=time()\n",
    "\n",
    "print( \"time diff {}\".format(t_end-t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros([2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=np.zeros(a.shape[1],b.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_mul(a,b):\n",
    "    c=np.zeros([a.shape[1],b.shape[0]])\n",
    "    if a.shape[1]==b.shape[0]:\n",
    "        for i in range(a.shape[0]):\n",
    "            for j in range(a.shape[1]):\n",
    "                val=0\n",
    "                for k in range(b.shape[1]):\n",
    "                    val=val+a[i][j]*b[j][k]\n",
    "                c[i,j]=val\n",
    "        print(c)        \n",
    "    else:\n",
    "        print(\"dim dont match\")\n",
    "                \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[[1,2],\n",
    "  [2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=[[2,3],\n",
    "  [2,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.ones([2,2])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=a*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_mul(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.random((100, 100))\n",
    "y = np.random.random((100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start=time()\n",
    "w=matrix_mul(x,y)\n",
    "t_end=time()\n",
    "\n",
    "print( \"time diff {}\".format(t_end-t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['imdb_score'].describe())\n",
    "sns.set_theme()\n",
    "sns.distplot(df['imdb_score'],bins=50,hist_kws={'alpha': 0.4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar=hist(df['imdb_score'],bins='freedman',)\n",
    "fig.clear(True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['imdb_score_bin']=pd.cut(df['imdb_score'],bins=len(ar[1])-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['imdb_score_bin']=df['imdb_score_bin'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols=['num_critic_for_reviews','duration','director_facebook_likes', 'actor_3_facebook_likes', \n",
    "       'actor_1_facebook_likes', 'gross','num_voted_users', 'cast_total_facebook_likes','budget', \n",
    "    'actor_2_facebook_likes','movie_facebook_likes']                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samp=df[['content_rating','imdb_score']].groupby(['content_rating']).agg(\"mean\")\n",
    "df_samp.reindex(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling NAs for a few Numerical features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(list(df.columns.values)not in cat_columns_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns=list(set(df.columns)-set(cat_columns_all))\n",
    "num_columns.remove('imdb_score_bin')\n",
    "num_columns.remove('imdb_score')\n",
    "num_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns=['director_facebook_likes',\n",
    " 'movie_facebook_likes',\n",
    " 'title_year',\n",
    " 'cast_total_facebook_likes',\n",
    " 'num_voted_users',\n",
    " 'num_critic_for_reviews',\n",
    " 'actor_3_facebook_likes',\n",
    " 'actor_1_facebook_likes',\n",
    " 'num_user_for_reviews',\n",
    " 'actor_2_facebook_likes',\n",
    " 'duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_mean=['director_facebook_likes','actor_3_facebook_likes','actor_1_facebook_likes',\n",
    "          'actor_2_facebook_likes','duration']\n",
    "\n",
    "for col in num_mean:\n",
    "    df.loc[df[col].isna(),col]=df[col].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_median=['title_year','duration']\n",
    "for col in num_median:\n",
    "    df.loc[df[col].isna(),col]=df[col].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_zero=['num_critic_for_reviews','num_user_for_reviews']\n",
    "for col in num_zero:\n",
    "    df.loc[df[col].isna(),col]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to plot the distribution of the independant variables\n",
    "\"\"\"\n",
    "def plot_dist(dataf=df,x_feat='content_rating',labels=None,fig=None,ax1=None):\n",
    "    if fig is None:\n",
    "        ax1 = sns.set_style(style=None, rc=None )\n",
    "        fig, ax1 = plt.subplots(figsize=(12,6))\n",
    "    if labels is None:\n",
    "        labels=list(dataf[x_feat].value_counts().index)\n",
    "    df_samp=dataf[[x_feat,'imdb_score']].groupby([x_feat]).agg(\"mean\")\n",
    "    plot=sns.countplot(data = dataf, x=x_feat, alpha=0.5, order=labels,ax=ax1,palette=\"summer\")\n",
    "    plot.set_xticklabels(labels, fontsize=9, rotation=30, ha= 'right')\n",
    "    ax1.tick_params(axis='y')\n",
    "    plt.ylabel(\"# Movies\")\n",
    "    plt.xlabel(x_feat)\n",
    "    ax2 = ax1.twinx()\n",
    "    sns.lineplot(data = df_samp['imdb_score'].values, marker='o', sort=False, ax=ax2)\n",
    "    total = len(df)\n",
    "    for p in plot.patches:\n",
    "        percentage = f'{100 * p.get_height() / total:.1f}%\\n'\n",
    "        x = p.get_x() + p.get_width() / 2\n",
    "        y = p.get_height()\n",
    "        plot.annotate(percentage, (x, y), ha='center', va='center',fontsize=7.5)\n",
    "    plot.axhline(df_samp['imdb_score'].mean())\n",
    "    plt.ylabel(\"IMDB rating\")\n",
    "    plt.title(\"Distribution of {}\".format(x_feat)) \n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to bin numerical variables before distribtion plot\n",
    "def bin_and_plot(num_columns):\n",
    "    bin_columns=[]\n",
    "    for col in num_columns:\n",
    "        colname=col+\"_bin\"\n",
    "        ar=hist(df[col],bins='freedman',)\n",
    "        if len(ar[1])<50:\n",
    "            num_bins= len(ar[1])-1\n",
    "        else:\n",
    "            num_bins=50\n",
    "        df[colname]=pd.cut(df[col],bins=num_bins)\n",
    "        bin_columns.append(colname)\n",
    "    plt.clf()\n",
    "\n",
    "    for colname in bin_columns:  \n",
    "        labels=list(map(str,df[colname].unique().sort_values()))\n",
    "        df[colname]=df[colname].astype(str)\n",
    "        plot_dist(df,x_feat=colname,labels=labels,fig=None,ax1=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id1.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Popularity of Movie and Director\n",
    "\n",
    "1. The popularity of the director and movie has a positive effect on the imdb ratings. Barring the first bin, others show a higher than average scores of >6.5. This is evident especially on the popularity of the movie.\n",
    "\n",
    "2. Directors name is not available for about 100 movies. Most of them are TV Series based on the ratings. Therefore there would be multiple directors for them. Creater would be a more equivalent role in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_columns=['director_facebook_likes', 'movie_facebook_likes']\n",
    "bin_and_plot(dir_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id1.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Popularity of Casts\n",
    "The overall total like of the cast didnt seem to have quite trend, the likes of actor_2 had a slight upward trend, barring in one group. We could say that having a strong/popular lead 2 might slightly increase the chance of a good movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_fb_columns=[ 'cast_total_facebook_likes',\n",
    " 'actor_1_facebook_likes',\n",
    " 'actor_2_facebook_likes', 'actor_3_facebook_likes']\n",
    "bin_and_plot(cast_fb_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id1.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Title year and Duration\n",
    "\n",
    "1. The older movies in the dataset have a comparatively better scores although their volumes are lower\n",
    "2. Most of movies that have scores below 5 have the duration around 1.5 hrs. The scores seem to be climbing up for movies that are upto 2hours 40 mins long. After that they dont seem to follow the trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df[~df['budget'].isna()],y='imdb_score',x='title_year')\n",
    "plt.title('Movie budget vs Imdb score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df[~df['budget'].isna()],y='imdb_score',x='duration',hue='language')\n",
    "plt.title('Movie budget vs Imdb score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_columns=['title_year','duration']\n",
    "bin_and_plot(td_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id1.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Budget and Profit\n",
    "\n",
    "1. The Budget field seemed to have outliers. It was interesting that all of the Top 10 movies of high budget was not from Hollywood. So this could mean the currencies were not normalized and they could be within their local currency. This may affect our models if not normalized by each country. For the purpose of this excercise, we could skip the non US made Movies for this reason. \n",
    "\n",
    "2. Movies with high budget and gross both seem to have higher scores. Although there are instances where good scores are there for smaller budget/gross movies.\n",
    "\n",
    "2. We could derive the profit the movie made by subtracting the budget from the gross. Most movies that have high profist also have high score. Soemtimes even if the movies didnt make much profit they still had good ratings\n",
    "\n",
    "3. Since the movies made from 1970s are included, we should account for inflation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df['budget'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['budget','country','movie_title']].sort_values(by='budget',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['gross','country','movie_title']].sort_values(by='gross',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hwd=df[df['country']=='USA']\n",
    "df_hwd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df_hwd[~df_hwd['budget'].isna()],x='imdb_score',y='budget')\n",
    "plt.title('Movie budget vs Imdb score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df_hwd[~df_hwd['gross'].isna()],x='imdb_score',y='gross')\n",
    "plt.title('Movie Gross vs Imdb score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hwd['profit']=df_hwd['gross']-df_hwd['budget']\n",
    "sns.scatterplot(data=df_hwd[~df_hwd['profit'].isna()],x='imdb_score',y='profit')\n",
    "plt.title('Movie profit vs Imdb score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id1.5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Analysis of other Movie features with response variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Assigning 'NA' for Null values.\n",
    "\n",
    "This can show if any of the Null values are correlated with the imdb scores, which would otherwise be missed. Since the plots would typically ignore these blank values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Black and White movies seem to be rated higher on an average. Although the volume is only around 4.1%\n",
    "\n",
    "\n",
    "2. Almost 93% of movies are in English. Some of the languages like Russian, Bosnian, Chinese have very low averaged\n",
    "\n",
    "3. The TV-MA (Mature in TV) content rating is way above the rest. Although less than 1% of movies are from that list. \n",
    "\n",
    "4. 6.5% of movies have aspect Ratio as NA which comparatively have lower ratings. Some of these movies seem to games based on movies or lesser known series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns_all=['director_name','actor_2_name','actor_1_name',\n",
    "                 'actor_3_name','plot_keywords','country',\n",
    "                 'movie_imdb_link','movie_title','genres','color',\n",
    "                 'facenumber_in_poster','language','content_rating',\n",
    "                 'aspect_ratio']\n",
    "\n",
    "for col in cat_columns_all:\n",
    "    df.loc[df[col].isna(),col]='NA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Considering only a few of the categorical columns for distribution analysis. Others do not make much sense for this plot\n",
    "cat_columns=['color','language','content_rating','aspect_ratio']\n",
    "df['aspect_ratio']=df['aspect_ratio'].astype(str)\n",
    "\n",
    "fig, ax = plt.subplots(round(len(cat_columns) / 2), 2, figsize = (20, 20))\n",
    "for i, ax in enumerate(fig.axes):\n",
    "    plot_dist(df,x_feat=cat_columns[i],fig=fig,ax1=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Cleaning and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Handling NAs\n",
    "\n",
    "1. Assigning color for movies that have NA since all are made after 1990\n",
    "2. Assign gross as budget and vice versa for values that have either of them. For both as blanks take the median\n",
    "3. Adjusted for inflation in gross, budget. rate is calculated as 2.0% based on some estimateions from https://smartasset.com/investing/inflation-calculator\n",
    "4. Handling NAs for other numeric fields are covered in the EDA section above\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['color']=='NA'),'color']='Color'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['aspect_ratio']=='NA'),'aspect_ratio']=0\n",
    "df['aspect_ratio']=df['aspect_ratio'].astype('Float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['budget']=np.where((df['budget'].isna() & (~df['gross'].isna())),df['gross'],df['budget'] )\n",
    "df['gross']=np.where((df['gross'].isna() & (~df['budget'].isna())),df['budget'],df['gross'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_median=['budget','gross']\n",
    "for col in num_median:\n",
    "    df.loc[df[col].isna(),col]=df[col].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['adj_year']=2016-df['title_year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id2.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Feature Engineering\n",
    "\n",
    "1. Profit of the movie from gross and budget\n",
    "2. Past imdb scores of directors and actors. Current scores are not included since it induces target leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inflation_corrected_amount(principle,  time,rate=2.0): \n",
    "    return(principle * (pow((1 + rate / 100), time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gross_adj']=df.apply(lambda x: inflation_corrected_amount(x['gross'],x['adj_year']),axis=1)\n",
    "df['budget_adj']=df.apply(lambda x: inflation_corrected_amount(x['budget'],x['adj_year']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['profit_adj']=df['gross_adj']-df['budget_adj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_past_score(name,year,field='director_name'):\n",
    "    val=df.loc[(df[field]==name) & (df['title_year']<year),'imdb_score'].mean()\n",
    "    if math.isnan(val):\n",
    "        return 0\n",
    "    else:\n",
    "        return val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['director_past_imdb_score']= df.apply(lambda x: get_past_score(x['director_name'],x['title_year']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['actor2_past_imdb_score']= df.apply(lambda x: get_past_score(x['actor_2_name'],\n",
    "                                                                x['title_year'],'actor_2_name'),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['actor1_past_imdb_score']= df.apply(lambda x: get_past_score(x['actor_1_name'],\n",
    "                                                                x['title_year'],'actor_1_name'),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['actor1_as_actor2_past_imdb_score']= df.apply(lambda x: get_past_score(x['actor_1_name'],\n",
    "                                                                x['title_year'],'actor_2_name'),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['actor2_as_actor1_past_imdb_score']= df.apply(lambda x: get_past_score(x['actor_2_name'],\n",
    "                                                                x['title_year'],'actor_1_name'),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_plot(df):\n",
    "    # Compute a correlation matrix and convert to long-form\n",
    "    corr_mat = df.corr().stack().reset_index(name=\"correlation\")\n",
    "\n",
    "    # Draw each cell as a scatter point with varying size and color\n",
    "    g = sns.relplot(\n",
    "        data=corr_mat,\n",
    "        x=\"level_0\", y=\"level_1\", hue=\"correlation\", size=\"correlation\",\n",
    "        palette=\"vlag\", hue_norm=(-1, 1), edgecolor=\".7\",\n",
    "        height=10, sizes=(50, 350), size_norm=(-.2, .6),\n",
    "    )\n",
    "\n",
    "    # Tweak the figure to finalize\n",
    "    g.set(xlabel=\"\", ylabel=\"\", aspect=\"equal\")\n",
    "    g.despine(left=True, bottom=True)\n",
    "    g.ax.margins(.02)\n",
    "    for label in g.ax.get_xticklabels():\n",
    "        label.set_rotation(90)\n",
    "    for artist in g.legend.legendHandles:\n",
    "        artist.set_edgecolor(\".7\")\n",
    "\n",
    "corr_plot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Baseline model\n",
    "\n",
    "The baseline model, we can predict all the movies that are above average as good movies. Sice our average score is 6.44 we can consider the scores that are above 7 as good movies and the remaining as not so good.\n",
    "Based on this we to get a baseline prediction which would predict all as the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['good_score']=0\n",
    "df.loc[df['imdb_score']>=7.0,'good_score']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['good_score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['baseline_predicted']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(df['good_score'],df['baseline_predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(df['good_score'],df['baseline_predicted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn import tree\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score,GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold,train_test_split\n",
    "from numpy import mean,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_metrics(m,x_test,y_true,y_pred):\n",
    "    plot_confusion_matrix(m,x_test,y_true,cmap=plt.cm.Blues)\n",
    "    plt.show()\n",
    "    acc=accuracy_score(y_pred=y_pred,y_true=y_true)\n",
    "    print('Accuracy score: {}'.format(acc))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df=df[['num_critic_for_reviews', 'duration',\n",
    "       'director_facebook_likes', 'actor_3_facebook_likes', \n",
    "       'actor_1_facebook_likes', \n",
    "        'num_voted_users', 'cast_total_facebook_likes', 'num_user_for_reviews', \n",
    "        'actor_2_facebook_likes',\n",
    "      'aspect_ratio', 'movie_facebook_likes', \n",
    "       'gross_adj', 'budget_adj', 'profit_adj', 'actor2_past_imdb_score','country',\n",
    "       'actor1_past_imdb_score', 'actor1_as_actor2_past_imdb_score',\n",
    "       'director_past_imdb_score', 'actor2_as_actor1_past_imdb_score', 'good_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = base_df.drop(['good_score','country'], axis = 1)\n",
    "y = base_df['good_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, test_size = 0.3, random_state = 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id4.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. Decision Tree\n",
    "\n",
    "THis is the simplest form of Tree which identifies rule which is then used to split and assign the leaf nodes to maximise our success metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train classifier\n",
    "clf = tree.DecisionTreeClassifier() \n",
    "clf=clf.fit(X_train, y_train) \n",
    "clf_prediction = clf.predict(X_test) \n",
    "get_model_metrics(clf,X_test,y_test, clf_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.plot_tree(clf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id4.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2. Random Forest\n",
    "Random forest is a bagging tree based model and usually it gives a good results because it is based on collection trees. Although the numbers are much better than our baseline and has predctions on both classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=RandomForestClassifier(n_estimators=200, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_y_pred = rf.predict(X_test)\n",
    "get_model_metrics(rf,X_test,y_test, rf_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id4.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3. Gradient Boost Method\n",
    "Sometimes the expection is that the xgboost model to have given better performance but is still comparable to the random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_y_pred = xgb_model.predict(X_test)\n",
    "get_model_metrics(xgb_model,X_test,y_test, xgb_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id4.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4. Catboost model\n",
    "The Cat boost model has gained popularity in the recently for its superior results and is known to be one of the best boosting algorithms.\n",
    "The results from catboost are also comparable to our xgboost and random forest model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_model = CatBoostClassifier(verbose=0, n_estimators=90)\n",
    "cat_model.fit(X_train, y_train)\n",
    "cat_y_pred = cat_model.predict(X_test)\n",
    "get_model_metrics(cat_model,X_test,y_test, cat_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Conclusion\n",
    "\n",
    "The features to making a sucessful imdb rated movies was explored. A baseline model was created and subsequent set of machine learning models were built that outperformed our baslien by a significant margin.\n",
    "\n",
    "In order to improve the scores, some of the additional features that were not inlcuded like the Title year, content rating etc could be tried out to check if they improve the scores. The budget field would need to normalized to reflect the uniform currency. Model Fuinetuning and grid search is also needed to improve the accuracy.\n",
    "\n",
    "Nevertheless our current model is able to predict a good movie with 80%+ accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
